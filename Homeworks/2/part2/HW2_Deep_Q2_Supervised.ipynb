{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "3EAanAcvZkdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.signal import convolve2d\n",
        "from torch import tensor, Tensor\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "H9Pswke6ZoKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required classes and Functions"
      ],
      "metadata": {
        "id": "3Mdu0s7IbCPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_y_neg(y):\n",
        "    y_neg = y.clone()\n",
        "    for idx, y_samp in enumerate(y):\n",
        "        allowed_indices = list(range(10))\n",
        "        allowed_indices.remove(y_samp.item())\n",
        "        y_neg[idx] = torch.tensor(allowed_indices)[\n",
        "            torch.randint(len(allowed_indices), size=(1,))\n",
        "        ].item()\n",
        "    return y_neg.to(device)\n",
        "\n",
        "\n",
        "def overlay_y_on_x(x, y, classes=10):\n",
        "    x_ = x.clone()\n",
        "    x_[:, :classes] *= 0.0\n",
        "    x_[range(x.shape[0]), y] = x.max()\n",
        "    return x_\n",
        "\n",
        "\n",
        "def get_metrics(preds, labels):\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return dict(accuracy_score=acc)\n",
        "\n",
        "\n",
        "class FF_Layer(nn.Linear):\n",
        "    def __init__(self, in_features: int, out_features: int, n_epochs: int, bias: bool, device):\n",
        "        super().__init__(in_features, out_features, bias=bias)\n",
        "        self.n_epochs = n_epochs\n",
        "        self.opt = torch.optim.Adam(self.parameters(),lr=0.03)\n",
        "        self.goodness = self.goodness_score\n",
        "        self.to(device)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "\n",
        "    def goodness_score(self, x_pos, x_neg, threshold=2):\n",
        "        g_pos = self(x_pos).pow(2).mean(1)\n",
        "        g_neg = self(x_neg).pow(2).mean(1)\n",
        "        loss = torch.log(1+ torch.exp(\n",
        "                      torch.cat([-g_pos + 2, g_neg - 2]))).mean()\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
        "        return self.relu(torch.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0))\n",
        "\n",
        "    def ff_train(self, pos_acts, neg_acts):\n",
        "\n",
        "        self.opt.zero_grad()\n",
        "        goodness = self.goodness(pos_acts, neg_acts)\n",
        "        goodness.backward()\n",
        "        self.opt.step()\n",
        "\n",
        "        return goodness.item()\n",
        "\n",
        "\n",
        "\n",
        "class Unsupervised_FF(nn.Module):\n",
        "    def __init__(self, n_layers: int = 2, n_neurons=500, input_size: int = 28 * 28, n_epochs: int = 100,\n",
        "                 bias: bool = True, n_classes: int = 10, n_hid_to_log: int = 2, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.n_hid_to_log = n_hid_to_log\n",
        "        self.n_epochs = n_epochs\n",
        "        self.device = device\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        ff_layers = [\n",
        "            FF_Layer(in_features=input_size if idx == 0 else n_neurons,\n",
        "                     out_features=n_neurons,\n",
        "                     n_epochs=n_epochs,\n",
        "                     bias=bias,\n",
        "                     device=device) for idx in range(n_layers)]\n",
        "\n",
        "        self.ff_layers = ff_layers\n",
        "\n",
        "    def train_ff_layers(self, pos_dataloader):\n",
        "        loss = [0]*self.n_layers\n",
        "        for epoch in range(self.n_epochs):\n",
        "            for i, pos_data in enumerate(pos_dataloader):\n",
        "                pos_imgs, labels = pos_data\n",
        "                pos_acts = torch.reshape(pos_imgs, (pos_imgs.shape[0], -1)).to(self.device)\n",
        "                pos_acts = overlay_y_on_x(pos_acts, labels)\n",
        "                neg_labels = get_y_neg(labels)\n",
        "                neg_acts = overlay_y_on_x(pos_acts, neg_labels)\n",
        "                if i%100 == 0:\n",
        "                  print(f'[loss layer 1, loss layer 2] = {loss}')\n",
        "                for idx, layer in enumerate(self.ff_layers):\n",
        "                    loss[idx] = layer.ff_train(pos_acts, neg_acts)\n",
        "                    pos_acts = layer(pos_acts)\n",
        "                    neg_acts = layer(neg_acts)\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate(self, dataloader: DataLoader, dataset_type: str = \"train\"):\n",
        "        self.eval()\n",
        "        all_labels = []\n",
        "        all_preds = []\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            x = torch.reshape(images, (images.shape[0], -1)).to(self.device)\n",
        "            goodness_per_label = []\n",
        "            for label in range(10):\n",
        "                h = overlay_y_on_x(x, label)\n",
        "                goodness = []\n",
        "                for layer in self.ff_layers:\n",
        "                    h = layer(h)\n",
        "                    goodness = goodness + [h.pow(2).mean(1)]\n",
        "                goodness_per_label += [sum(goodness).unsqueeze(1)]\n",
        "            goodness_per_label = torch.cat(goodness_per_label, 1)\n",
        "            return goodness_per_label.argmax(1)\n",
        "\n"
      ],
      "metadata": {
        "id": "diBqCU95bK-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model and Showing the results"
      ],
      "metadata": {
        "id": "6Ukm27pIbLue"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6n5i7wbK7WX",
        "outputId": "4fecc766-d1eb-443b-c26a-94b92e03c2b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[loss layer 1, loss layer 2] = [0, 0]\n",
            "[loss layer 1, loss layer 2] = [0.695438027381897, 0.6961270570755005]\n",
            "[loss layer 1, loss layer 2] = [0.631627082824707, 0.6682161688804626]\n",
            "[loss layer 1, loss layer 2] = [0.5928605794906616, 0.6224926710128784]\n",
            "[loss layer 1, loss layer 2] = [0.5431445837020874, 0.5708013772964478]\n",
            "[loss layer 1, loss layer 2] = [0.4694785475730896, 0.49569588899612427]\n",
            "[loss layer 1, loss layer 2] = [0.4455793797969818, 0.4524715840816498]\n",
            "[loss layer 1, loss layer 2] = [0.4109407067298889, 0.4268926978111267]\n",
            "[loss layer 1, loss layer 2] = [0.38530591130256653, 0.3999406099319458]\n",
            "[loss layer 1, loss layer 2] = [0.369320273399353, 0.390566349029541]\n",
            "[loss layer 1, loss layer 2] = [0.36510998010635376, 0.3598301410675049]\n",
            "[loss layer 1, loss layer 2] = [0.35682758688926697, 0.38679641485214233]\n",
            "[loss layer 1, loss layer 2] = [0.30811697244644165, 0.3351837992668152]\n",
            "[loss layer 1, loss layer 2] = [0.30782175064086914, 0.3316900432109833]\n",
            "[loss layer 1, loss layer 2] = [0.3094712793827057, 0.3066282272338867]\n",
            "[loss layer 1, loss layer 2] = [0.2996090054512024, 0.29924678802490234]\n",
            "[loss layer 1, loss layer 2] = [0.2909083962440491, 0.29617491364479065]\n",
            "[loss layer 1, loss layer 2] = [0.2678162157535553, 0.2651212513446808]\n",
            "[loss layer 1, loss layer 2] = [0.25446248054504395, 0.24690282344818115]\n",
            "[loss layer 1, loss layer 2] = [0.27594655752182007, 0.28012603521347046]\n",
            "[loss layer 1, loss layer 2] = [0.2628089189529419, 0.2697758674621582]\n",
            "[loss layer 1, loss layer 2] = [0.26391875743865967, 0.28064820170402527]\n",
            "[loss layer 1, loss layer 2] = [0.2509695589542389, 0.24866656959056854]\n",
            "[loss layer 1, loss layer 2] = [0.2441394329071045, 0.24477718770503998]\n",
            "[loss layer 1, loss layer 2] = [0.23794716596603394, 0.24803917109966278]\n",
            "[loss layer 1, loss layer 2] = [0.2499164342880249, 0.23832595348358154]\n",
            "[loss layer 1, loss layer 2] = [0.24436068534851074, 0.23787984251976013]\n",
            "[loss layer 1, loss layer 2] = [0.19981735944747925, 0.19568698108196259]\n",
            "[loss layer 1, loss layer 2] = [0.22222492098808289, 0.20318594574928284]\n",
            "[loss layer 1, loss layer 2] = [0.22383378446102142, 0.22819183766841888]\n",
            "[loss layer 1, loss layer 2] = [0.22339175641536713, 0.19271105527877808]\n",
            "[loss layer 1, loss layer 2] = [0.2102421671152115, 0.21983331441879272]\n",
            "[loss layer 1, loss layer 2] = [0.21162796020507812, 0.2125089466571808]\n",
            "[loss layer 1, loss layer 2] = [0.22617515921592712, 0.2353304624557495]\n",
            "[loss layer 1, loss layer 2] = [0.1802593618631363, 0.1938580572605133]\n",
            "[loss layer 1, loss layer 2] = [0.17584356665611267, 0.20167143642902374]\n",
            "[loss layer 1, loss layer 2] = [0.16538743674755096, 0.19977156817913055]\n",
            "[loss layer 1, loss layer 2] = [0.1801307201385498, 0.21577399969100952]\n",
            "[loss layer 1, loss layer 2] = [0.1757541298866272, 0.22775176167488098]\n",
            "[loss layer 1, loss layer 2] = [0.18559420108795166, 0.21010999381542206]\n",
            "[loss layer 1, loss layer 2] = [0.17881172895431519, 0.2037263661623001]\n",
            "[loss layer 1, loss layer 2] = [0.19987507164478302, 0.24030721187591553]\n",
            "[loss layer 1, loss layer 2] = [0.16547724604606628, 0.2008087933063507]\n",
            "[loss layer 1, loss layer 2] = [0.18147359788417816, 0.22402483224868774]\n",
            "[loss layer 1, loss layer 2] = [0.14444509148597717, 0.18995177745819092]\n",
            "[loss layer 1, loss layer 2] = [0.14476551115512848, 0.1752786636352539]\n",
            "[loss layer 1, loss layer 2] = [0.17079690098762512, 0.21008670330047607]\n",
            "[loss layer 1, loss layer 2] = [0.18177911639213562, 0.23176072537899017]\n",
            "[loss layer 1, loss layer 2] = [0.13830506801605225, 0.19213604927062988]\n",
            "[loss layer 1, loss layer 2] = [0.15663015842437744, 0.21843190491199493]\n",
            "[loss layer 1, loss layer 2] = [0.16241607069969177, 0.21501879394054413]\n",
            "[loss layer 1, loss layer 2] = [0.19317227602005005, 0.23687103390693665]\n",
            "[loss layer 1, loss layer 2] = [0.1394629329442978, 0.19497813284397125]\n",
            "[loss layer 1, loss layer 2] = [0.16661235690116882, 0.21754798293113708]\n",
            "[loss layer 1, loss layer 2] = [0.17048728466033936, 0.22593438625335693]\n",
            "[loss layer 1, loss layer 2] = [0.1485971063375473, 0.2137344777584076]\n",
            "[loss layer 1, loss layer 2] = [0.1495356261730194, 0.20918959379196167]\n",
            "[loss layer 1, loss layer 2] = [0.14120686054229736, 0.1871335506439209]\n",
            "[loss layer 1, loss layer 2] = [0.1610681414604187, 0.2166895866394043]\n",
            "[loss layer 1, loss layer 2] = [0.12839600443840027, 0.18791069090366364]\n",
            "[loss layer 1, loss layer 2] = [0.15865035355091095, 0.20032113790512085]\n",
            "[loss layer 1, loss layer 2] = [0.13573020696640015, 0.1820775866508484]\n",
            "[loss layer 1, loss layer 2] = [0.14085598289966583, 0.19224214553833008]\n",
            "[loss layer 1, loss layer 2] = [0.1492505669593811, 0.20419412851333618]\n",
            "[loss layer 1, loss layer 2] = [0.1428208202123642, 0.20142461359500885]\n",
            "[loss layer 1, loss layer 2] = [0.13400551676750183, 0.2083500474691391]\n",
            "[loss layer 1, loss layer 2] = [0.13606128096580505, 0.21024450659751892]\n",
            "[loss layer 1, loss layer 2] = [0.12578435242176056, 0.21190787851810455]\n",
            "[loss layer 1, loss layer 2] = [0.13883192837238312, 0.16824686527252197]\n",
            "[loss layer 1, loss layer 2] = [0.11948809027671814, 0.19591979682445526]\n",
            "[loss layer 1, loss layer 2] = [0.14785346388816833, 0.20233549177646637]\n",
            "[loss layer 1, loss layer 2] = [0.13804645836353302, 0.1819748878479004]\n",
            "[loss layer 1, loss layer 2] = [0.11959826946258545, 0.15822242200374603]\n",
            "[loss layer 1, loss layer 2] = [0.13106754422187805, 0.18908706307411194]\n",
            "[loss layer 1, loss layer 2] = [0.13163043558597565, 0.16361114382743835]\n",
            "[loss layer 1, loss layer 2] = [0.11956994235515594, 0.13676345348358154]\n",
            "[loss layer 1, loss layer 2] = [0.14510701596736908, 0.20388688147068024]\n",
            "[loss layer 1, loss layer 2] = [0.13780751824378967, 0.19015514850616455]\n",
            "[loss layer 1, loss layer 2] = [0.1222449541091919, 0.15570782124996185]\n",
            "[loss layer 1, loss layer 2] = [0.12430111318826675, 0.1793610155582428]\n",
            "[loss layer 1, loss layer 2] = [0.13402816653251648, 0.16569790244102478]\n",
            "[loss layer 1, loss layer 2] = [0.11167880892753601, 0.15878430008888245]\n",
            "[loss layer 1, loss layer 2] = [0.11840154230594635, 0.16166706383228302]\n",
            "[loss layer 1, loss layer 2] = [0.12151660770177841, 0.18961231410503387]\n",
            "[loss layer 1, loss layer 2] = [0.11513452231884003, 0.19799122214317322]\n",
            "[loss layer 1, loss layer 2] = [0.11817733198404312, 0.16111797094345093]\n",
            "[loss layer 1, loss layer 2] = [0.12421388924121857, 0.18358325958251953]\n",
            "[loss layer 1, loss layer 2] = [0.1544511616230011, 0.20522408187389374]\n",
            "[loss layer 1, loss layer 2] = [0.10607878863811493, 0.15187552571296692]\n",
            "[loss layer 1, loss layer 2] = [0.12239986658096313, 0.17149223387241364]\n",
            "[loss layer 1, loss layer 2] = [0.10283465683460236, 0.13776008784770966]\n",
            "[loss layer 1, loss layer 2] = [0.11371473968029022, 0.1616145223379135]\n",
            "[loss layer 1, loss layer 2] = [0.13247892260551453, 0.1855703592300415]\n",
            "[loss layer 1, loss layer 2] = [0.10701747983694077, 0.136565700173378]\n",
            "[loss layer 1, loss layer 2] = [0.13678550720214844, 0.16746461391448975]\n",
            "[loss layer 1, loss layer 2] = [0.11674541234970093, 0.18161091208457947]\n",
            "[loss layer 1, loss layer 2] = [0.12350501865148544, 0.1874232143163681]\n",
            "[loss layer 1, loss layer 2] = [0.11430122703313828, 0.1672067940235138]\n",
            "[loss layer 1, loss layer 2] = [0.15071648359298706, 0.19399890303611755]\n",
            "[loss layer 1, loss layer 2] = [0.11612293124198914, 0.15236060321331024]\n",
            "[loss layer 1, loss layer 2] = [0.1346149444580078, 0.18304315209388733]\n",
            "[loss layer 1, loss layer 2] = [0.12027145177125931, 0.16996653378009796]\n",
            "[loss layer 1, loss layer 2] = [0.12581492960453033, 0.15728339552879333]\n",
            "[loss layer 1, loss layer 2] = [0.13377511501312256, 0.19733567535877228]\n",
            "[loss layer 1, loss layer 2] = [0.12033835053443909, 0.18093517422676086]\n",
            "[loss layer 1, loss layer 2] = [0.11253108829259872, 0.19665658473968506]\n",
            "[loss layer 1, loss layer 2] = [0.11674104630947113, 0.16190758347511292]\n",
            "[loss layer 1, loss layer 2] = [0.1218813955783844, 0.18390725553035736]\n",
            "[loss layer 1, loss layer 2] = [0.11254197359085083, 0.13327603042125702]\n",
            "[loss layer 1, loss layer 2] = [0.11325748264789581, 0.1726464033126831]\n",
            "[loss layer 1, loss layer 2] = [0.10990940034389496, 0.15220129489898682]\n",
            "[loss layer 1, loss layer 2] = [0.13476964831352234, 0.14169031381607056]\n",
            "[loss layer 1, loss layer 2] = [0.11364874988794327, 0.15679723024368286]\n",
            "[loss layer 1, loss layer 2] = [0.11208807677030563, 0.16204950213432312]\n",
            "[loss layer 1, loss layer 2] = [0.09174057841300964, 0.1467703878879547]\n",
            "[loss layer 1, loss layer 2] = [0.09850633144378662, 0.14856818318367004]\n",
            "[loss layer 1, loss layer 2] = [0.09958160668611526, 0.14918667078018188]\n",
            "[loss layer 1, loss layer 2] = [0.09701046347618103, 0.10626950114965439]\n",
            "[loss layer 1, loss layer 2] = [0.11664311587810516, 0.1507965475320816]\n",
            "[loss layer 1, loss layer 2] = [0.10561752319335938, 0.13947346806526184]\n",
            "[loss layer 1, loss layer 2] = [0.12023763358592987, 0.13765305280685425]\n",
            "[loss layer 1, loss layer 2] = [0.11319535970687866, 0.14153501391410828]\n",
            "[loss layer 1, loss layer 2] = [0.10918033123016357, 0.12597370147705078]\n",
            "[loss layer 1, loss layer 2] = [0.10722410678863525, 0.12305110692977905]\n",
            "[loss layer 1, loss layer 2] = [0.1138039082288742, 0.16655367612838745]\n",
            "[loss layer 1, loss layer 2] = [0.10997874289751053, 0.1514137089252472]\n",
            "[loss layer 1, loss layer 2] = [0.10445530712604523, 0.15860909223556519]\n",
            "[loss layer 1, loss layer 2] = [0.10174466669559479, 0.12134765088558197]\n",
            "[loss layer 1, loss layer 2] = [0.10082609951496124, 0.13566145300865173]\n",
            "[loss layer 1, loss layer 2] = [0.09451326727867126, 0.13857832551002502]\n",
            "[loss layer 1, loss layer 2] = [0.09937311708927155, 0.13219475746154785]\n",
            "[loss layer 1, loss layer 2] = [0.09172160923480988, 0.1371760368347168]\n",
            "[loss layer 1, loss layer 2] = [0.14948676526546478, 0.18658575415611267]\n",
            "[loss layer 1, loss layer 2] = [0.09883955121040344, 0.1308085024356842]\n",
            "[loss layer 1, loss layer 2] = [0.09653755277395248, 0.14452168345451355]\n",
            "[loss layer 1, loss layer 2] = [0.09685327857732773, 0.11025257408618927]\n",
            "[loss layer 1, loss layer 2] = [0.12877541780471802, 0.17470186948776245]\n",
            "[loss layer 1, loss layer 2] = [0.10328049957752228, 0.14562420547008514]\n",
            "[loss layer 1, loss layer 2] = [0.09379490464925766, 0.13165351748466492]\n",
            "[loss layer 1, loss layer 2] = [0.1236800104379654, 0.1495533138513565]\n",
            "[loss layer 1, loss layer 2] = [0.10076981782913208, 0.1116497814655304]\n",
            "[loss layer 1, loss layer 2] = [0.10042022168636322, 0.1398007571697235]\n",
            "[loss layer 1, loss layer 2] = [0.0917801484465599, 0.12188833206892014]\n",
            "[loss layer 1, loss layer 2] = [0.10794499516487122, 0.14150182902812958]\n",
            "[loss layer 1, loss layer 2] = [0.104990154504776, 0.15676383674144745]\n",
            "[loss layer 1, loss layer 2] = [0.08869198709726334, 0.12968814373016357]\n",
            "[loss layer 1, loss layer 2] = [0.101207435131073, 0.12368984520435333]\n",
            "[loss layer 1, loss layer 2] = [0.081527940928936, 0.11022554337978363]\n",
            "[loss layer 1, loss layer 2] = [0.09127255529165268, 0.12755528092384338]\n",
            "[loss layer 1, loss layer 2] = [0.10555772483348846, 0.15662260353565216]\n",
            "[loss layer 1, loss layer 2] = [0.09560917317867279, 0.15847420692443848]\n",
            "[loss layer 1, loss layer 2] = [0.11020836234092712, 0.139902263879776]\n",
            "[loss layer 1, loss layer 2] = [0.09781283140182495, 0.15556976199150085]\n",
            "[loss layer 1, loss layer 2] = [0.09159135073423386, 0.13054797053337097]\n",
            "[loss layer 1, loss layer 2] = [0.09454074501991272, 0.12051285803318024]\n",
            "[loss layer 1, loss layer 2] = [0.10397905111312866, 0.1421206295490265]\n",
            "[loss layer 1, loss layer 2] = [0.12135498225688934, 0.14990131556987762]\n",
            "[loss layer 1, loss layer 2] = [0.09349817782640457, 0.13621103763580322]\n",
            "[loss layer 1, loss layer 2] = [0.09969979524612427, 0.13136571645736694]\n",
            "[loss layer 1, loss layer 2] = [0.11515450477600098, 0.13760249316692352]\n",
            "[loss layer 1, loss layer 2] = [0.10233409702777863, 0.12688344717025757]\n",
            "[loss layer 1, loss layer 2] = [0.10880687087774277, 0.1464567482471466]\n",
            "[loss layer 1, loss layer 2] = [0.1020740270614624, 0.11958132684230804]\n",
            "[loss layer 1, loss layer 2] = [0.088210828602314, 0.11487115174531937]\n",
            "[loss layer 1, loss layer 2] = [0.08634987473487854, 0.120112344622612]\n",
            "[loss layer 1, loss layer 2] = [0.09637542814016342, 0.12364297360181808]\n",
            "[loss layer 1, loss layer 2] = [0.09271353483200073, 0.13404884934425354]\n",
            "[loss layer 1, loss layer 2] = [0.09078434109687805, 0.12608695030212402]\n",
            "[loss layer 1, loss layer 2] = [0.0884438082575798, 0.11378978192806244]\n",
            "[loss layer 1, loss layer 2] = [0.0897156372666359, 0.12014851719141006]\n",
            "[loss layer 1, loss layer 2] = [0.08820977061986923, 0.1103971004486084]\n",
            "[loss layer 1, loss layer 2] = [0.10256554186344147, 0.13537050783634186]\n",
            "[loss layer 1, loss layer 2] = [0.09247185289859772, 0.13083350658416748]\n",
            "[loss layer 1, loss layer 2] = [0.09831210225820541, 0.13843274116516113]\n",
            "[loss layer 1, loss layer 2] = [0.08951787650585175, 0.13075312972068787]\n",
            "[loss layer 1, loss layer 2] = [0.08741044998168945, 0.11386279761791229]\n",
            "[loss layer 1, loss layer 2] = [0.10844410955905914, 0.15046747028827667]\n",
            "[loss layer 1, loss layer 2] = [0.08169272541999817, 0.10957956314086914]\n",
            "[loss layer 1, loss layer 2] = [0.09463601559400558, 0.13194234669208527]\n",
            "[loss layer 1, loss layer 2] = [0.09161268174648285, 0.13208559155464172]\n",
            "[loss layer 1, loss layer 2] = [0.08636371791362762, 0.0997840166091919]\n",
            "[loss layer 1, loss layer 2] = [0.09922893345355988, 0.13253910839557648]\n",
            "[loss layer 1, loss layer 2] = [0.09698823094367981, 0.13360720872879028]\n",
            "[loss layer 1, loss layer 2] = [0.10047483444213867, 0.1498008817434311]\n",
            "[loss layer 1, loss layer 2] = [0.08903035521507263, 0.11922634392976761]\n",
            "[loss layer 1, loss layer 2] = [0.0975809246301651, 0.15379087626934052]\n",
            "[loss layer 1, loss layer 2] = [0.1446637213230133, 0.22350536286830902]\n",
            "[loss layer 1, loss layer 2] = [0.11154752969741821, 0.14212216436862946]\n",
            "[loss layer 1, loss layer 2] = [0.09012669324874878, 0.10993073880672455]\n",
            "[loss layer 1, loss layer 2] = [0.09006046503782272, 0.11855920404195786]\n",
            "[loss layer 1, loss layer 2] = [0.08085530996322632, 0.10334715992212296]\n",
            "[loss layer 1, loss layer 2] = [0.09589007496833801, 0.1372426152229309]\n",
            "[loss layer 1, loss layer 2] = [0.10121770948171616, 0.1189967691898346]\n",
            "[loss layer 1, loss layer 2] = [0.08574236184358597, 0.12299329042434692]\n",
            "[loss layer 1, loss layer 2] = [0.08759217709302902, 0.11045888066291809]\n",
            "[loss layer 1, loss layer 2] = [0.08393688499927521, 0.11708494275808334]\n",
            "[loss layer 1, loss layer 2] = [0.09175807237625122, 0.11685050278902054]\n",
            "[loss layer 1, loss layer 2] = [0.08615107834339142, 0.1103174090385437]\n",
            "[loss layer 1, loss layer 2] = [0.09680293500423431, 0.11989440768957138]\n",
            "[loss layer 1, loss layer 2] = [0.09009495377540588, 0.11788655817508698]\n",
            "[loss layer 1, loss layer 2] = [0.08446025848388672, 0.1152617335319519]\n",
            "[loss layer 1, loss layer 2] = [0.09241361916065216, 0.0867920070886612]\n",
            "[loss layer 1, loss layer 2] = [0.08685106039047241, 0.12017309665679932]\n",
            "[loss layer 1, loss layer 2] = [0.09032104909420013, 0.12236964702606201]\n",
            "[loss layer 1, loss layer 2] = [0.11463595926761627, 0.14740024507045746]\n",
            "[loss layer 1, loss layer 2] = [0.09350498765707016, 0.11753710359334946]\n",
            "[loss layer 1, loss layer 2] = [0.09422839432954788, 0.12022334337234497]\n",
            "[loss layer 1, loss layer 2] = [0.08312889188528061, 0.10335969924926758]\n",
            "[loss layer 1, loss layer 2] = [0.08900906145572662, 0.1136307492852211]\n",
            "[loss layer 1, loss layer 2] = [0.09066102653741837, 0.11460763216018677]\n",
            "[loss layer 1, loss layer 2] = [0.08437372744083405, 0.10198706388473511]\n",
            "[loss layer 1, loss layer 2] = [0.08366367220878601, 0.10057415068149567]\n",
            "[loss layer 1, loss layer 2] = [0.08623336255550385, 0.11513538658618927]\n",
            "[loss layer 1, loss layer 2] = [0.07973960041999817, 0.10710545629262924]\n",
            "[loss layer 1, loss layer 2] = [0.09886084496974945, 0.10447964072227478]\n",
            "[loss layer 1, loss layer 2] = [0.0838531106710434, 0.12531565129756927]\n",
            "[loss layer 1, loss layer 2] = [0.07813584804534912, 0.11910609900951385]\n",
            "[loss layer 1, loss layer 2] = [0.09175840765237808, 0.09813978523015976]\n",
            "[loss layer 1, loss layer 2] = [0.08380723744630814, 0.11356331408023834]\n",
            "[loss layer 1, loss layer 2] = [0.08617206662893295, 0.10025780647993088]\n",
            "[loss layer 1, loss layer 2] = [0.0864860862493515, 0.12618565559387207]\n",
            "[loss layer 1, loss layer 2] = [0.08242041617631912, 0.11312452703714371]\n",
            "[loss layer 1, loss layer 2] = [0.0793362408876419, 0.09808900207281113]\n",
            "[loss layer 1, loss layer 2] = [0.08751338720321655, 0.11170881986618042]\n",
            "[loss layer 1, loss layer 2] = [0.0909644290804863, 0.12790828943252563]\n",
            "[loss layer 1, loss layer 2] = [0.07871999591588974, 0.11173669993877411]\n",
            "[loss layer 1, loss layer 2] = [0.08965855091810226, 0.1139555349946022]\n",
            "[loss layer 1, loss layer 2] = [0.08907976746559143, 0.11893220990896225]\n",
            "[loss layer 1, loss layer 2] = [0.1066971868276596, 0.12859587371349335]\n",
            "[loss layer 1, loss layer 2] = [0.08494339138269424, 0.09568041563034058]\n",
            "[loss layer 1, loss layer 2] = [0.08174625039100647, 0.10270468145608902]\n",
            "[loss layer 1, loss layer 2] = [0.07865344732999802, 0.10419821739196777]\n",
            "[loss layer 1, loss layer 2] = [0.08711719512939453, 0.12488681823015213]\n",
            "[loss layer 1, loss layer 2] = [0.08207617700099945, 0.0972079336643219]\n",
            "[loss layer 1, loss layer 2] = [0.10472016036510468, 0.1651797890663147]\n",
            "[loss layer 1, loss layer 2] = [0.10000085830688477, 0.13069021701812744]\n",
            "[loss layer 1, loss layer 2] = [0.08000864833593369, 0.10065700113773346]\n",
            "[loss layer 1, loss layer 2] = [0.09011735767126083, 0.1345943957567215]\n",
            "[loss layer 1, loss layer 2] = [0.0881667509675026, 0.14354099333286285]\n",
            "[loss layer 1, loss layer 2] = [0.08619840443134308, 0.10506617277860641]\n",
            "[loss layer 1, loss layer 2] = [0.09111146628856659, 0.14612939953804016]\n",
            "[loss layer 1, loss layer 2] = [0.081562839448452, 0.0986793115735054]\n",
            "[loss layer 1, loss layer 2] = [0.07671007513999939, 0.09702295064926147]\n",
            "[loss layer 1, loss layer 2] = [0.0837724432349205, 0.12155666202306747]\n",
            "[loss layer 1, loss layer 2] = [0.08659698814153671, 0.12575596570968628]\n",
            "[loss layer 1, loss layer 2] = [0.08635789155960083, 0.10821113735437393]\n",
            "[loss layer 1, loss layer 2] = [0.12014628946781158, 0.13454195857048035]\n",
            "[loss layer 1, loss layer 2] = [0.08990182727575302, 0.12999726831912994]\n",
            "[loss layer 1, loss layer 2] = [0.08160495012998581, 0.10185645520687103]\n",
            "[loss layer 1, loss layer 2] = [0.08016625046730042, 0.1020866259932518]\n",
            "[loss layer 1, loss layer 2] = [0.08624386787414551, 0.12290899455547333]\n",
            "[loss layer 1, loss layer 2] = [0.07894176244735718, 0.10659186542034149]\n",
            "[loss layer 1, loss layer 2] = [0.08509653061628342, 0.09621711820363998]\n",
            "[loss layer 1, loss layer 2] = [0.0795031487941742, 0.09783121943473816]\n",
            "[loss layer 1, loss layer 2] = [0.09175776690244675, 0.11177629232406616]\n",
            "[loss layer 1, loss layer 2] = [0.0841483324766159, 0.09616189450025558]\n",
            "[loss layer 1, loss layer 2] = [0.07925530523061752, 0.09890703856945038]\n",
            "[loss layer 1, loss layer 2] = [0.08781634271144867, 0.11507326364517212]\n",
            "[loss layer 1, loss layer 2] = [0.10960176587104797, 0.12083357572555542]\n",
            "[loss layer 1, loss layer 2] = [0.07602494210004807, 0.08993111550807953]\n",
            "[loss layer 1, loss layer 2] = [0.08922526240348816, 0.11972439289093018]\n",
            "[loss layer 1, loss layer 2] = [0.07935603708028793, 0.08928726613521576]\n",
            "[loss layer 1, loss layer 2] = [0.07787195593118668, 0.09866037964820862]\n",
            "[loss layer 1, loss layer 2] = [0.09608598053455353, 0.12342755496501923]\n",
            "[loss layer 1, loss layer 2] = [0.09186204522848129, 0.11403957009315491]\n",
            "[loss layer 1, loss layer 2] = [0.08151459693908691, 0.09764711558818817]\n",
            "[loss layer 1, loss layer 2] = [0.1054309755563736, 0.11148776859045029]\n",
            "[loss layer 1, loss layer 2] = [0.08119497448205948, 0.09619554132223129]\n",
            "[loss layer 1, loss layer 2] = [0.07450029253959656, 0.09962555766105652]\n",
            "[loss layer 1, loss layer 2] = [0.0843476802110672, 0.13595254719257355]\n",
            "[loss layer 1, loss layer 2] = [0.08262154459953308, 0.10372760891914368]\n",
            "[loss layer 1, loss layer 2] = [0.0862313061952591, 0.10205535590648651]\n",
            "[loss layer 1, loss layer 2] = [0.07834026217460632, 0.09604798257350922]\n",
            "[loss layer 1, loss layer 2] = [0.07978272438049316, 0.10898074507713318]\n",
            "[loss layer 1, loss layer 2] = [0.07645478844642639, 0.09729701280593872]\n",
            "[loss layer 1, loss layer 2] = [0.07740409672260284, 0.10254806280136108]\n",
            "[loss layer 1, loss layer 2] = [0.08685649931430817, 0.10865335166454315]\n",
            "[loss layer 1, loss layer 2] = [0.08241158723831177, 0.10058308392763138]\n",
            "[loss layer 1, loss layer 2] = [0.08377735316753387, 0.08999823778867722]\n",
            "[loss layer 1, loss layer 2] = [0.10739727318286896, 0.12994036078453064]\n",
            "[loss layer 1, loss layer 2] = [0.07666416466236115, 0.09385965764522552]\n",
            "[loss layer 1, loss layer 2] = [0.07234306633472443, 0.10172470659017563]\n",
            "[loss layer 1, loss layer 2] = [0.08112208545207977, 0.12100717425346375]\n",
            "[loss layer 1, loss layer 2] = [0.08607134968042374, 0.10091158747673035]\n",
            "[loss layer 1, loss layer 2] = [0.08831825852394104, 0.11081866919994354]\n",
            "[loss layer 1, loss layer 2] = [0.07635706663131714, 0.117035411298275]\n",
            "[loss layer 1, loss layer 2] = [0.08179448544979095, 0.11210526525974274]\n",
            "[loss layer 1, loss layer 2] = [0.07652030885219574, 0.1201302632689476]\n",
            "[loss layer 1, loss layer 2] = [0.07556817680597305, 0.10674253851175308]\n",
            "[loss layer 1, loss layer 2] = [0.079335056245327, 0.10296635329723358]\n",
            "[loss layer 1, loss layer 2] = [0.07917176932096481, 0.12899357080459595]\n",
            "[loss layer 1, loss layer 2] = [0.07485944032669067, 0.10537143051624298]\n",
            "[loss layer 1, loss layer 2] = [0.08490419387817383, 0.11421822011470795]\n",
            "[loss layer 1, loss layer 2] = [0.0819375216960907, 0.09231173247098923]\n",
            "[loss layer 1, loss layer 2] = [0.08481115847826004, 0.13357502222061157]\n",
            "[loss layer 1, loss layer 2] = [0.08393888920545578, 0.11612770706415176]\n",
            "[loss layer 1, loss layer 2] = [0.07674171030521393, 0.09268657863140106]\n",
            "[loss layer 1, loss layer 2] = [0.08570923656225204, 0.11102097481489182]\n",
            "[loss layer 1, loss layer 2] = [0.08435336500406265, 0.10308802127838135]\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "pos_dataset = torchvision.datasets.MNIST(root='./', download=True, transform=transform, train=True)\n",
        "pos_dataloader = DataLoader(pos_dataset, batch_size=256, shuffle=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./', train=False, download=True, transform=transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=10000)\n",
        "\n",
        "\n",
        "unsupervised_ff = Unsupervised_FF(device=device, n_epochs=100)\n",
        "unsupervised_ff.train_ff_layers(pos_dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(pos_dataset, batch_size=50000)\n",
        "x_train, y_train = next(iter(train_dataloader))\n",
        "x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "x_te, y_te = next(iter(test_dataloader))\n",
        "x_te, y_te = x_te.to(device), y_te.to(device)\n",
        "print(\"train accuracy:\", unsupervised_ff.evaluate(train_dataloader).eq(y_train).float().mean().item())\n",
        "print(\"test accuracy:\", unsupervised_ff.evaluate(test_dataloader).eq(y_te).float().mean().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGxUhXcoib9f",
        "outputId": "77ccceaf-33cc-4ef2-ae54-a04824c5f00b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train accuracy: 0.9863199591636658\n",
            "test accuracy: 0.9734999537467957\n"
          ]
        }
      ]
    }
  ]
}