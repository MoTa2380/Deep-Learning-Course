# Deep Learning Course Repository 

This repository contains materials and assignments for the Deep Learning course. Below is an outline of the key topics covered in the course:

## 1. [Introduction to Machine Learning](https://github.com/MoTa2380/Deep-Learning-Course/blob/main/slides/(02)%20-%20Machine%20Learning%20Basics%20-%202023.pdf)
The course begins with an overview of machine learning, explaining the different types:
- **Supervised Learning**
- **Unsupervised Learning**
- **Reinforcement Learning**

It introduces common machine learning models, including:
- k-Nearest Neighbors (KNN)
- Naive Bayes Classifier
- Logistic Regression
- Decision Trees
- Support Vector Machines (SVM)
- Ensemble Models

## 2. [Mathematical Prerequisites](https://github.com/MoTa2380/Deep-Learning-Course/blob/main/slides/(03)%20-%20Essential%20Math%20for%20ML%20-%202023.pdf)
Key mathematical foundations required for the course include:
- **Linear Algebra**
- **Probability and Statistics**

## 3. [Multilayer Perceptrons (MLP) and Backpropagation](https://github.com/MoTa2380/Deep-Learning-Course/blob/main/slides/(04)%20-%20Perceptron%20%20MLP%20-%202023%20(1).pdf)
The course covers the basics of Multilayer Perceptrons (MLPs) and the backpropagation algorithm used for training neural networks.

## 4. [Regularization and Optimization Techniques](https://github.com/MoTa2380/Deep-Learning-Course/blob/main/slides/(05)%20-%20Regularization%20and%20Optimization%20-%202023.pdf)
Various regularization techniques are discussed, such as:
- **L2 and L1 Regularization**
- **Data Augmentation**
- **Dropout**

Additionally, optimization methods include:
- **Stochastic Gradient Descent (SGD)**
- **Adam Optimizer**
- **Batch Normalization**

## 5. [Convolutional Neural Networks (CNNs)](https://github.com/MoTa2380/Deep-Learning-Course/blob/main/slides/(06)%20-%20CNN%20Architectures%20-%202023.pdf)
The course delves into CNN architectures and popular models such as:
- **AlexNet**
- **ResNet**
- **GoogleNet**
- **VGGNet**

[Applications in computer vision](https://github.com/MoTa2380/Deep-Learning-Course/blob/main/slides/(07)%20-%20Computer%20Vision%20Application%20-%202023.pdf) are also covered, such as:
- **Object Detection** with Faster R-CNN, YOLO
- **Image Segmentation** with SegNet

## 6. [Recurrent Neural Networks (RNNs) and Natural Language Processing (NLP)](https://github.com/MoTa2380/Deep-Learning-Course/blob/main/slides/(08)%20-%20RNN%20and%20LSTM%20-%202023.pdf)
Key concepts in RNNs and NLP include:
- **RNNs and LSTMs**
- **Tokenization**
- **Word Embeddings** like Word2Vec
- **Transformers and Attention Mechanisms**

## 7. [Autoencoders and Variational Autoencoders (VAE)](https://github.com/MoTa2380/Deep-Learning-Course/blob/main/slides/(09)%20-%20AE%20and%20VAE%20-%202023.pdf)
Different types of Autoencoders are introduced, including:
- **Denoising Autoencoders (DAE)**
- **Stacked Autoencoders (SAE)**
- **Sparse Autoencoders (SAE)**

Applications of Autoencoders in feature reduction, denoising, and compression are discussed, along with **Variational Autoencoders (VAE)** such as:
- Conditional VAE
- Hierarchical VAE
- VQ-VAE

## 8. [Generative Adversarial Networks (GANs)](https://github.com/MoTa2380/Deep-Learning-Course/blob/main/slides/(10)%20-%20GAN%20-%202023.pdf)
The course covers GANs and their challenges, such as mode collapse and vanishing gradients. Different types of GANs are explored, including:
- **Deep Convolutional GAN (DCGAN)**
- **Conditional GAN (cGAN)**
- **CycleGAN**
- **Wasserstein GAN (WGAN)**
- **StyleGAN**
